{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Algoritmos para Big Data\n",
    "\n",
    "**Handout 4 - Binary classification**\n",
    "\n",
    "**2024/25**\n",
    "\n",
    "This lab class is about binary classification in a discrete space. We will setup a ML processing\n",
    "pipeline to achieve the goals, and the data to be considered relates to the domain of banking industry.\n",
    "Specifically, it is a case of fraud detection in credit cards transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task A - Data ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drop duplicates - no final\n",
    "tirar 3 colunas fora -> as que têm nulls: state, zip e errors\n",
    "dropNA\n",
    "tirar dollar da coluna -> criar nova coluna com dinheiro limpo\n",
    "aí sim, ficheiro em condições"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SparkSession\n",
    "spark = SparkSession.builder.appName(\"Features\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading and checking data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User,Card,Year,Month,Day,Time,Amount,Use Chip,Merchant Name,Merchant City,Merchant State,Zip,MCC,Errors?,Is Fraud?\n",
      "0,0,2002,9,1,06:21,$134.09,Swipe Transaction,3527213246127876953,La Verne,CA,91750.0,5300,,No\n",
      "0,0,2002,9,1,06:42,$38.48,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,2,06:22,$120.34,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,2,17:45,$128.95,Swipe Transaction,3414527459579106770,Monterey Park,CA,91754.0,5651,,No\n",
      "0,0,2002,9,3,06:23,$104.71,Swipe Transaction,5817218446178736267,La Verne,CA,91750.0,5912,,No\n",
      "0,0,2002,9,3,13:53,$86.19,Swipe Transaction,-7146670748125200898,Monterey Park,CA,91755.0,5970,,No\n",
      "0,0,2002,9,4,05:51,$93.84,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,4,06:09,$123.50,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n",
      "0,0,2002,9,5,06:14,$61.72,Swipe Transaction,-727612092139916043,Monterey Park,CA,91754.0,5411,,No\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "data_dir = '../datasets/'\n",
    "file = data_dir + 'credit-cards-transactions.csv'\n",
    "\n",
    "! head $file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cards = spark.read.csv(file, header=True, sep=',', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "|User|Card|Year|Month|Day|               Time| Amount|         Use Chip|       Merchant Name|Merchant City|Merchant State|    Zip| MCC|Errors?|Is Fraud?|\n",
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "|   0|   0|2002|    9|  1|2025-03-27 06:21:00|$134.09|Swipe Transaction| 3527213246127876953|     La Verne|            CA|91750.0|5300|   NULL|       No|\n",
      "|   0|   0|2002|    9|  1|2025-03-27 06:42:00| $38.48|Swipe Transaction| -727612092139916043|Monterey Park|            CA|91754.0|5411|   NULL|       No|\n",
      "|   0|   0|2002|    9|  2|2025-03-27 06:22:00|$120.34|Swipe Transaction| -727612092139916043|Monterey Park|            CA|91754.0|5411|   NULL|       No|\n",
      "|   0|   0|2002|    9|  2|2025-03-27 17:45:00|$128.95|Swipe Transaction| 3414527459579106770|Monterey Park|            CA|91754.0|5651|   NULL|       No|\n",
      "|   0|   0|2002|    9|  3|2025-03-27 06:23:00|$104.71|Swipe Transaction| 5817218446178736267|     La Verne|            CA|91750.0|5912|   NULL|       No|\n",
      "|   0|   0|2002|    9|  3|2025-03-27 13:53:00| $86.19|Swipe Transaction|-7146670748125200898|Monterey Park|            CA|91755.0|5970|   NULL|       No|\n",
      "|   0|   0|2002|    9|  4|2025-03-27 05:51:00| $93.84|Swipe Transaction| -727612092139916043|Monterey Park|            CA|91754.0|5411|   NULL|       No|\n",
      "|   0|   0|2002|    9|  4|2025-03-27 06:09:00|$123.50|Swipe Transaction| -727612092139916043|Monterey Park|            CA|91754.0|5411|   NULL|       No|\n",
      "|   0|   0|2002|    9|  5|2025-03-27 06:14:00| $61.72|Swipe Transaction| -727612092139916043|Monterey Park|            CA|91754.0|5411|   NULL|       No|\n",
      "|   0|   0|2002|    9|  5|2025-03-27 09:35:00| $57.10|Swipe Transaction| 4055257078481058705|     La Verne|            CA|91750.0|7538|   NULL|       No|\n",
      "+----+----+----+-----+---+-------------------+-------+-----------------+--------------------+-------------+--------------+-------+----+-------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "df_cards - number of rows: 24386900\n",
      "root\n",
      " |-- User: integer (nullable = true)\n",
      " |-- Card: integer (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- Day: integer (nullable = true)\n",
      " |-- Time: timestamp (nullable = true)\n",
      " |-- Amount: string (nullable = true)\n",
      " |-- Use Chip: string (nullable = true)\n",
      " |-- Merchant Name: long (nullable = true)\n",
      " |-- Merchant City: string (nullable = true)\n",
      " |-- Merchant State: string (nullable = true)\n",
      " |-- Zip: double (nullable = true)\n",
      " |-- MCC: integer (nullable = true)\n",
      " |-- Errors?: string (nullable = true)\n",
      " |-- Is Fraud?: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking data\n",
    "df_cards.show(10)\n",
    "print(f'df_cards - number of rows: {df_cards.count()}')\n",
    "df_cards.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_amount(amount):\n",
    "    return float(amount.split(\"$\")[1])\n",
    "\n",
    "udf_amount = F.udf(get_amount, T.FloatType())\n",
    "\n",
    "\n",
    "\n",
    "df_plot = ( df_cards\n",
    "           .groupby('Year')\n",
    "           .count()\n",
    "           .sort('Year', ascending=True)\n",
    "           .toPandas()\n",
    "           )\n",
    "\n",
    "fig = px.bar(df_plot, title='Number of transactions by year',\n",
    "             x='Year', y='count')\n",
    "fig.show()\n",
    "\n",
    "\n",
    "\n",
    "print('Uniqueness of values:')\n",
    "nr_of_records = df_cards.count()\n",
    "cols_interest = df_cards.columns\n",
    "for col in cols_interest:\n",
    "    k = df_cards.select(col).distinct().count()\n",
    "    print(f'Column {col}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "org.apache.spark.util does not exist in the JVM",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdf_cards - number of rows is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf_cards\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m; after dropDuplicates() applied would be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdf_cards\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropDuplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/sql/dataframe.py:1240\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1218\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1219\u001b[0m \n\u001b[1;32m   1220\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:181\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 181\u001b[0m     converted \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_exception\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m         \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:173\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PythonException(msg, stacktrace)\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mUnknownException\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstackTrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstacktrace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcause\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:65\u001b[0m, in \u001b[0;36mCapturedException.__init__\u001b[0;34m(self, desc, stackTrace, cause, origin)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstackTrace \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     61\u001b[0m     stackTrace\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stackTrace \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (SparkContext\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(origin))\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcause\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m cause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m origin\u001b[38;5;241m.\u001b[39mgetCause() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcause \u001b[38;5;241m=\u001b[39m convert_exception(origin\u001b[38;5;241m.\u001b[39mgetCause())\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:157\u001b[0m, in \u001b[0;36mconvert_exception\u001b[0;34m(e)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkUpgradeException(origin\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m    156\u001b[0m c: Py4JJavaError \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39mgetCause()\n\u001b[0;32m--> 157\u001b[0m stacktrace: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mjvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutil\u001b[49m\u001b[38;5;241m.\u001b[39mUtils\u001b[38;5;241m.\u001b[39mexceptionString(e)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    159\u001b[0m     is_instance_of(gw, c, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.spark.api.python.PythonException\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# To make sure this only catches Python UDFs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m     )\n\u001b[1;32m    166\u001b[0m ):\n\u001b[1;32m    167\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  An exception was thrown from the Python worker. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see the stack trace below.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m c\u001b[38;5;241m.\u001b[39mgetMessage()\n\u001b[1;32m    170\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/vscode_pyspark/lib/python3.11/site-packages/py4j/java_gateway.py:1664\u001b[0m, in \u001b[0;36mJavaPackage.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaClass(\n\u001b[1;32m   1662\u001b[0m         answer[proto\u001b[38;5;241m.\u001b[39mCLASS_FQN_START:], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client)\n\u001b[1;32m   1663\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1664\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m does not exist in the JVM\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(new_fqn))\n",
      "\u001b[0;31mPy4JError\u001b[0m: org.apache.spark.util does not exist in the JVM"
     ]
    }
   ],
   "source": [
    "print(f'df_cards - number of rows is {df_cards.count()}; after dropDuplicates() applied would be {df_cards.dropDuplicates().count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'''df_cards - number of rows after dropna(how='any') applied would be {df_cards.dropna(how='any').count()}.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Checking nulls at each column of df_cards')\n",
    "dict_nulls_iot = {col: df_cards.filter(df_cards[col].isNull()).count() for col in df_cards.columns}\n",
    "dict_nulls_iot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a dataframe with columns of interest, as well as arrays with names of columns to look at later on\n",
    "\n",
    "# column 'device': use F.regexp_replace() on column 'device_name' (use F.col()), replacing '-' by ' '.\n",
    "# column 'device_words': use F.split() on column 'device_name' (use F.col()) by '-'\n",
    "\n",
    "df_devices = ( df_cards\n",
    "            .withColumn('device', F.regexp_replace(F.col('device_name'), '-', ' '))\n",
    "            .withColumn('device_words', F.split(F.col('device_name'), '\\-'))\n",
    "            .select('device_id','device', 'device_words', 'battery_level', 'c02_level', 'humidity', 'temp', 'cn')\n",
    ")\n",
    "df_devices.show(10,truncate=False)\n",
    "\n",
    "# call describe on df_devices and show\n",
    "df_devices.describe().show()\n",
    "\n",
    "# numeric columns\n",
    "input_cols_num = ['battery_level', 'c02_level', 'humidity', 'temp']\n",
    "# string columns\n",
    "input_cols_str = ['cn']\n",
    "# all interest columns together\n",
    "input_cols_all = input_cols_num + input_cols_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots (histograms) to grasp data\n",
    "\n",
    "# df_devices\n",
    "# input_cols_num = ['battery_level', 'c02_level', 'humidity', 'temp']\n",
    "# input_cols_str = ['cn']\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "df_pandas = df_devices.toPandas()\n",
    "\n",
    "id = 2\n",
    "# col = 'cn'\n",
    "col = input_cols_num[id]\n",
    "fig = px.histogram(df_pandas, x=col)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task B - Basic statistics\n",
    "\n",
    "Applying the following statistical algorithms upon the dataframe of interest:\n",
    "- Correlation, with help of feature transformer VectorAssembler\n",
    "- Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations among numeric columns\n",
    "#\n",
    "# Correlation needs vectors so we convert to vector column first\n",
    "# See VectorAssembler\n",
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# The columns to compute correlations - numeric types but no nulls\n",
    "cols_corr = input_cols_num\n",
    "\n",
    "# Assemble columns\n",
    "col_features = \"features\"\n",
    "assembler = VectorAssembler(inputCols=cols_corr, outputCol=col_features, handleInvalid = \"skip\")\n",
    "# Apply transform on df_devices an select col_futures \n",
    "df_features = assembler.transform(df_devices)\n",
    "    \n",
    "\n",
    "df_features.show(10, truncate=False)\n",
    "\n",
    "# Get correlation matrix - it can be Pearson’s (default) or Spearman’s correlation\n",
    "corr_matrix = Correlation.corr(df_features, col_features).collect()[0][0].toArray().tolist()\n",
    "\n",
    "corr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot computed correlation\n",
    "fig = px.imshow(corr_matrix, text_auto=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizer\n",
    "from pyspark.ml.stat import Summarizer\n",
    "\n",
    "summarizer = Summarizer.metrics('min', 'max', 'mean', 'sum', 'count', 'variance', 'std', 'normL1', 'normL2')\n",
    "\n",
    "df_features.show(10, truncate=False)\n",
    "\n",
    "print('Aggregated metric below:\\n')\n",
    "df_features.select(summarizer.summary(df_features.features)).show(truncate=False)\n",
    "\n",
    "print('Single metrics below:\\n')\n",
    "df_features.select(Summarizer.min(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.max(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.mean(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.sum(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.count(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.variance(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.std(df_features.features)).show(truncate=False)\n",
    "df_features.select(Summarizer.normL2(df_features.features)).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task C - Features extraction\n",
    "\n",
    "Extracting features frow raw data, according to the following algorithms:\n",
    "- TF-IDF\n",
    "- Word2Vec\n",
    "- CountVectorizer\n",
    "- FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_devices.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"device_words\", outputCol=\"rawFeatures\", numFeatures=10) \n",
    "hashing_df = hashingTF.transform(df_devices) \n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"featuresTFIDF\") \n",
    "idf_model = idf.fit(hashing_df) \n",
    "\n",
    "tfidf_df = idf_model.transform(hashing_df) \n",
    "tfidf_df.select('device_id','device_words','featuresTFIDF').show(10,truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(vectorSize=4, minCount=0, inputCol=\"device_words\", outputCol=\"features\") \n",
    "word2vec_model = word2vec.fit(df_devices)\n",
    "word2vec_df = word2vec_model.transform(df_devices)\n",
    "word2vec_df.select(\"device_id\", \"device_words\", \"features\").show(10,truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(inputCol=\"device_words\", outputCol=\"features\", vocabSize=3) \n",
    "cv_model = cv.fit(df_devices)\n",
    "cv_df = cv_model.transform(df_devices)\n",
    "cv_df.select(\"device_id\",'device_words', \"features\").show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FeatureHasher\n",
    "from pyspark.ml.feature import FeatureHasher \n",
    "\n",
    "hasher = FeatureHasher(inputCols=input_cols_all, outputCol='features', numFeatures=4) \n",
    "hasher_df = hasher.transform(df_devices)\n",
    "cols_to_show = ['device_id','device'] + input_cols_all + ['features']\n",
    "hasher_df.select(cols_to_show).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task D - Feature transformation\n",
    "\n",
    "Modifying features into more suitable formats, according to the following algorithms: \n",
    "- Tokenizer\n",
    "- StringIndexer\n",
    "- OneHotEncoder\n",
    "- Binarizer\n",
    "- Bucketizer\n",
    "- StandardScaler\n",
    "- MinMaxScaler\n",
    "- PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer(inputCol='device', outputCol='device_tokens')\n",
    "df_tokenizer = tokenizer.transform(df_devices)\n",
    "df_tokenizer.select('device_id', 'device', 'device_words', 'device_tokens').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StringIndexer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"cn\", outputCol=\"cn_indexed\") \n",
    "indexer_model = indexer.fit(df_devices)\n",
    "indexer_df = indexer_model.transform(df_devices)\n",
    "indexer_df.select('device_id', 'device', 'cn', 'cn_indexed').show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "incols = ['battery_level', 'c02_level']\n",
    "outcols = ['battery_level_vec', 'c02_level_vec']\n",
    "encoder = OneHotEncoder(inputCols=incols,outputCols=outcols) \n",
    "encoder_model = encoder.fit(df_devices)\n",
    "encoder_df = encoder_model.transform(df_devices)\n",
    "cols_to_show = ['device_id', 'device'] + incols + outcols\n",
    "encoder_df.select(cols_to_show).show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarizer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# As operator requires numeric data but not integer, we have to create a new but adequate column\n",
    "df = df_devices.select('device_id', 'device','battery_level').withColumn('battery_level_d', F.col('battery_level').cast('double'))\n",
    "\n",
    "threshold_cut = 5.0\n",
    "binarizer = Binarizer(threshold=threshold_cut, inputCol='battery_level_d', outputCol='battery_level_binary') \n",
    "binarizer_df = binarizer.transform(df)\n",
    "print(f'Binarizer output with threshold {threshold_cut}:') \n",
    "binarizer_df.show(10, truncate=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketizer\n",
    "from pyspark.ml.feature import Bucketizer \n",
    " \n",
    "splits = [0.0, 2.0, 4.0, float('inf')] \n",
    "bucketizer = Bucketizer(splits=splits, inputCol='battery_level', outputCol='battery_level_bucket') \n",
    "bucketizer_df = bucketizer.transform(df_devices)\n",
    "print(f'Bucketizer output with {(len(bucketizer.getSplits()) - 1)} buckets:') \n",
    "bucketizer_df.select('device_id', 'device', 'battery_level', 'battery_level_bucket').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardScaler\n",
    "from pyspark.ml.feature import StandardScaler \n",
    "\n",
    "# Assemble columns\n",
    "col_features = 'features'\n",
    "col_features_scaled = 'features_scaled' \n",
    "assembler = VectorAssembler(inputCols=input_cols_num, outputCol=col_features, handleInvalid = \"skip\") # \"keep\"\n",
    "df_features = assembler.transform(df_devices)\n",
    "\n",
    "scaler = StandardScaler(inputCol=col_features, outputCol=col_features_scaled, withStd=True, withMean=True) \n",
    "scaler_model = scaler.fit(df_features)\n",
    "scaler_df = scaler_model.transform(df_features)\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_scaled]\n",
    "scaler_df.select(cols_to_show).show(10,truncate=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMaxScaler\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "#col_features = 'features'\n",
    "#col_features_scaled = 'features_scaled' \n",
    "#assembler = VectorAssembler(inputCols=input_cols_num, outputCol=col_features, handleInvalid = \"skip\") # \"keep\"\n",
    "#df_features = assembler.transform(df_devices)\n",
    "\n",
    "# Use of col_features, col_features_scaled, and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=col_features, outputCol=col_features_scaled) \n",
    "scaler_model = scaler.fit(df_features)\n",
    "scaler_df = scaler_model.transform(df_features)\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_scaled]\n",
    "print(f'Features scaled to range [{scaler.getMin()}, {scaler.getMax()}]:') \n",
    "scaler_df.select(cols_to_show).show(10,truncate=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "from pyspark.ml.feature import PCA \n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "col_features_pca = 'features_PCA'\n",
    "pca = PCA(k=2, inputCol=col_features, outputCol=col_features_pca) \n",
    "pca_model = pca.fit(df_features)\n",
    "pca_df = pca_model.transform(df_features)\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_features_pca] \n",
    "pca_df.select(cols_to_show).show(10,truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Task E - Feature selection\n",
    "\n",
    "Selecting a relevant subset of features from a larger set of features\n",
    "- ChiSqSelector\n",
    "- VectorSlicer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChiSqSelector\n",
    "from pyspark.ml.feature import ChiSqSelector \n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "# Add a label column, such as danger = 1 if battery_level < 2 || c02_level > 1000 || temp > 26; 0 otherwise\n",
    "df = df_features.withColumn('danger', \n",
    "            F.when((F.col('battery_level') < 2) | (F.col('c02_level') > 1000) | (F.col('temp') > 26), 1.0)\n",
    "            .otherwise(0.0)\n",
    "        )\n",
    "df.show()\n",
    "\n",
    "col_selected_features = 'selected_features'\n",
    "col_label = 'danger'\n",
    "selector = ChiSqSelector(numTopFeatures=2, featuresCol=col_features, outputCol=col_selected_features, labelCol=col_label) \n",
    "selector_model = selector.fit(df)\n",
    "selector_df = selector_model.transform(df)\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_label, col_selected_features] \n",
    "selector_df.select(cols_to_show).show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VectorSlicer\n",
    "from pyspark.ml.feature import VectorSlicer\n",
    "\n",
    "# Use of col_features and df_features \n",
    "# from previous exercise StandardScaler\n",
    "\n",
    "col_selected_features = 'selected_features'\n",
    "# indices: humidity -> 2, temp -> 3\n",
    "slicer = VectorSlicer(inputCol=col_features, outputCol=col_selected_features, indices=[2,3]) \n",
    "slicer_df = slicer.transform(df_features)\n",
    "\n",
    "cols_to_show = ['device_id','device'] + input_cols_num + [col_features, col_selected_features] \n",
    "slicer_df.select(cols_to_show).show(10,truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vscode_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
